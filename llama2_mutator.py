# PPO process that receive seeds from fuzzer then send back the mutated seeds to fuzzer
from dataclasses import dataclass, field
from typing import Optional
import os
import torch
import tyro
from accelerate import Accelerator
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, BitsAndBytesConfig

from trl import (
    AutoModelForCausalLMWithValueHead,
    PPOConfig,
    set_seed,
)
import threading
import re
import sysv_ipc
import struct
import random

tqdm.pandas()

TYPE_SEED = 1
TYPE_TEXT_SEED = 2
TYPE_REWARD = 3
TYPE_REQUEST = 4

access_token = "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"
cur_path = os.path.dirname(os.path.realpath(__file__))
output_dir = os.path.join(cur_path, "ppo_checkpoint")
message_queue = []
seed_id_map = {}
id_rwd_map = {}
seeds_from_fuzzer = set()
uid = 1
shared_resource_lock = threading.Lock()


@dataclass
class ScriptArguments:
    """
    Setup experiment config
    """

    fuzzing_target: Optional[str] = field(default="libpng")
    fuzzing_object: Optional[str] = field(default="")
    if_mixed_model: Optional[bool] = field(default=True)
    if_text: Optional[bool] = field(default=False)
    temperature: Optional[float] = field(default=1.25)
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=64,
            lora_alpha=16,
            lora_dropout=0.1,
            bias="none",
            target_modules=[
                "q_proj",
                "down_proj",
                "gate_proj",
                "o_proj",
                "k_proj",
                "v_proj",
                "up_proj",
            ],
            task_type="CAUSAL_LM",
        ),
    )
    trust_remote_code: bool = field(
        default=True, metadata={"help": "Enable `trust_remote_code`"}
    )


args = tyro.cli(ScriptArguments)


def mq_thread2():
    try:
        mq2 = sysv_ipc.MessageQueue(1234, sysv_ipc.IPC_CREAT)
    except sysv_ipc.ExistentialError:
        print(f"Message queue with key {1234} already exists.")
        return
    while True:
        # only receive request msg
        try:
            msg, mtype = mq2.receive(type=TYPE_REQUEST)
        except RuntimeError as e:
            print(e)


def mq_thread():
    """
    Thread to receive request from fuzzer, and send generated seed to fuzzer
    """
    global message_queue, seed_id_map, seeds_from_fuzzer
    try:
        mq = sysv_ipc.MessageQueue(1234, sysv_ipc.IPC_CREAT)
    except sysv_ipc.ExistentialError:
        print(f"Message queue with key {1234} already exists.")
        return
    while True:
        # only receive request msg
        try:
            msg, mtype = mq.receive(type=TYPE_REQUEST)
            if msg != b"":
                if len(seeds_from_fuzzer) > 30:
                    seeds_from_fuzzer.clear()
                seeds_from_fuzzer.add(msg.decode(errors="ignore")[4:])
            if message_queue != []:
                # send uid + seed
                seed = message_queue.pop(0)
                send_msg = struct.pack("I", seed_id_map[seed])
                if len(seed) > (2045 - len(send_msg)):
                    seed = seed[: (2045 - len(send_msg))]
                send_msg = send_msg + seed.encode("utf-8")
                if len(send_msg) >= 2045:
                    print("::oversize")
                    continue
                if not args.if_text:
                    mq.send(
                        send_msg,
                        True,
                        type=TYPE_SEED,
                    )
                else:
                    mq.send(
                        send_msg,
                        True,
                        type=TYPE_TEXT_SEED,
                    )
        except RuntimeError as e:
            print(e)


def hex_string_to_hex(hex_string, fuzzing_target, if_text):
    """
    Formatting generated hex string.

    Returns:
        String of hex.
    """
    if len(hex_string.split("### Output:")) >= 2:
        hex_string = hex_string.split("### Output:")[1]
    else:
        hex_string = hex_string.replace(
            f"### Input: ```Based on below hex {fuzzing_target} seed, mutate a new {fuzzing_target} seed. Make sure the example is complete and valid.",
            " ",
        )
    if not if_text:
        hex_string = re.sub(r"[^a-zA-Z0-9\s]", " ", hex_string)
        hex_values = hex_string.replace("0x", " ")
        # Split the string into sections
        sections = hex_values.split()
        # Iterate through the sections and add leading zeros if needed
        result = []
        for section in sections:
            if len(section) == 1:
                section = "0" + section
                result.append(section)
            elif len(section) == 2:
                result.append(section)
        result = "".join(result)
    else:
        result = hex_string
    if len(result) > 2040:  # limite seed size to 2048
        result = result[:2040]
    return result


def main():
    """
    Main function to run PPO loop
    """
    model_name = f"llama-2-7b-structured-{args.fuzzing_target}-hex-mutator"
    if args.if_mixed_model:
        model_name = f"llama-2-7b-structured-{args.fuzzing_target}-mix-hex-mutator"
    if args.fuzzing_object != "":
        model_name = f"llama-2-7b-structured-{args.fuzzing_target}-{args.fuzzing_object}-mix-hex-mutator"
        args.fuzzing_target = args.fuzzing_target + " " + args.fuzzing_object
    # Init the tokenizer and dataset
    tokenizer = AutoTokenizer.from_pretrained(
        os.path.join(cur_path, model_name),
        use_fast=True,
        token=access_token,
    )
    # Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
    # tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.pad_token = tokenizer.bos_token
    # tokenizer.padding_side = "left"
    # We retrieve the dataloader by calling the `build_dataset` function.

    # set seed before initializing value head for deterministic eval
    set_seed(0)

    # Build the model.
    peft_config = args.peft_config
    # Copy the model to each device
    current_device = Accelerator().local_process_index
    device_map = {"": current_device}

    model = AutoModelForCausalLMWithValueHead.from_pretrained(
        os.path.join(cur_path, model_name),
        trust_remote_code=args.trust_remote_code,
        device_map=device_map,
        peft_config=peft_config,
        token=access_token,
        torch_dtype=torch.bfloat16,
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        ),
        # use_flash_attention_2=True, Unable to use this feature in current GPU
    )
    # Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.
    model.config.use_cache = False
    model.config.pretraining_tp = 1

    # flash attention 1
    torch.backends.cuda.sdp_kernel(
        enable_flash=True, enable_math=False, enable_mem_efficient=False
    )
    example = {
        "kamailio parse_msg": "0x670x6f,0x640x76,0x710x6e,0x780x66,0x6a0x69,0x700xe9,0x10x1,0x10x87,0x6e0x20,0x700xef,0xe10x83,0x00x1c,0x10x6c,0x200x53,0x590x5a,0x00x0,0x00x0,0x00xa,0x2d0x2d,0x6a0x6d,0x740x74,0x3a0x2c,0x2e0x2e,0x2c0xf6,0x920x16,0x10x33,0x350x4f,0x10xdd,0x5c0x59,0x10x1,0x820x2c,0x200x46,0xf0x20,0x00x4b,0x510x33,0x200x20,0x200x20,0x200x20,0x170x87,0x200x4b,0x4f0x35,0xa0x6d,0x3a0x2c,0x2c0x2c,0x2c0xee,0x7a0x79,0x630x6b,0x760x9,0xd0x4b,0x520x70,0x4f0x34,0x690xbf,0xbf0xcd,0x7f0x70,0x90x9,0x90x9,0x10x1,0x520x4a,0x320xa,0x560x3a,0x90x9,0x90x9,0x90x9,0x530x49,0x500x2f,0x320x2e,0x300x20,0x90x2f,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x770x77,0x200x20,0x200x20,0x200x52,0x470x34,0x670x6f,0x640x76,0x710x6e,0x740x3a,0x2d0x2e,0x2d0x2e,0x2e0x2d,0x6a0x73,0x670x74,0x3a0xa,0xa0xf,0x10x58,0x510x8,0x00x4e,0x770x56,0x00xa2,0x5e0x56,0x00x0,0xfd0x9,0x90x9,0x90x9,0x90x89,0x6f0x17,0x540x4c,0x390xa,0x740x3a,0x2d0x6b,0x720x79,0x200x62,0x790x6f,0x770x71,0x6b0x0,0x870x61,0x2c0x2c,0xfd0x9,0x90x9,0x90x68,0x69",
        "kamailio uri": "0x750x72,0x6e0x3a,0x760x75,0x710x6e,0x740x3b,0x540x72,0x610x6e,0x730x70,0x6f0x40,0x3a0x2c,0x440x44,0x560x70,0x790xed,0x3c0x76,0x790x3b,0x610x3a,0x5e0x3a,0x3b0x74,0x3b0x6a,0x3a0x3f,0xba0x2d",
        "binutils nm": "0x4d0x5a,0xef0x1,0x1c0x1,0x890x0,0x30x0,0x10x1,0x00x0,0x10x1,0x00x0,0x00x0,0x00x0,0x00xc,0x70xbc,0x6c0x80,0xa00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x900x74,0x3e0x1,0x00x0,0x00x0,0xa00x6d,0x10x1,0x10x1,0x00x0,0x10x1,0x10x0,0x800x0,0x00x0,0xe0x1,0xba0xe,0x10xcb,0x10x0,0x10x1,0xe0x4b,0xcd0x2a,0x570x75,0x760x6c,0x200x75,0x790x76,0x6b0x6f,0x610x71,0x200x63,0x6c0x70,0x6b0x6d,0x790x20,0x660x62,0x200x7a,0x6f0x68,0x200x69,0x770x20,0x420x52,0x510x20,0x700x78,0x610x1,0x00xd,0xd0xa,0x230x0,0x00x0,0x00x0,0x00x0,0x500x45,0x00x0,0x1d0xfd,0x10x0,0x900x79,0x2a0x1,0x70x0,0x00x0,0x1c0x0,0x00x0,0x70x0,0x10x1,0x1b0x1,0x00x1c,0x10x91,0x3d0x1,0x00x1,0x10x1,0x10x1,0x10x0,0x00x0,0x00x0,0x00x0,0x00x1,0x00x0,0x00x1,0x10x1,0x10x0,0x00x1,0x10x0,0x00x0,0x00x0,0xef0x1,0x10x0,0x100x2,0x110x1,0x310x11,0x90x67,0x10x1,0x10xef,0x10x1,0x10x1,0x00x0,0x00x0,0x720x61,0x20x5e,0x10x6,0x50x5,0x20x0,0x20x3,0x200x1b,0x90x1,0x00x0,0x00x0,0x00x0,0x240x0,0x00x0,0x00x0,0x00xa,0x00x0,0x00xc,0x10x0,0x00x82,0x00x1,0x10x1,0x10x0,0x00x0,0x00x0,0x00x1,0x10x0,0x00x0,0x00x24,0x00x0,0x00x0,0x00x0,0x00x24,0x00x0,0x00x0,0x00x0,0x00x24,0x00x0,0x00x0,0x00x0,0x00x24,0x00x0,0x00x14,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x1a,0x00x0,0x00x0,0x00x0,0x00x0,0x240x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00xd2,0x00x0,0x00x0,0x00x0,0x00x0,0x00x1,0x00x0,0x00x2,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x2b,0x00x0,0x10x1,0x00x0,0x00xd,0x00x0,0x00x13,0x00x0,0x00x9d,0x00x1,0x10x0,0x10x0,0x00x0,0x00x0,0x00xe6,0x30x0,0x00x1,0x00x19,0x00x0,0x00x0,0x00x0,0x00x0,0x00x80,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x3,0x00xe,0x00x1,0x10x0,0xbb0xbb,0xbb0xbb,0xbb0xbb,0xbb0xbb,0xbb0xbb,0xbb0x0,0x10x0,0x00x4,0x00x1c,0xbb0xbb,0xbb0xbb,0xbb0xbb,0xbb0xbb,0x00x17,0xf00x9e,0x1c0x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x3,0x00x10,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x1,0x10x1,0x10x1,0x10x1c,0x10xd,0x10x1,0x10x1,0x10x1,0x00x0,0x00x0,0x00x0,0x00x0,0x310x0,0x00x0,0x00x11,0x00x0,0x1",
        "binutils objcopy": "0x750x1,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x8,0x50x20,0x00x0,0x00x0,0x00x47",
        "binutils readelf": "0x7f0x45,0x4c0x46,0x7f0x46,0x4e0x43,0x90x0,0x00x0,0x00x22,0x3e0x71,0x670x36,0x20x1,0xd0x0,0x00x0,0x800x0,0x00x0,0x10x90,0xd30xa5,0x40x0,0x00x0,0x10x1,0x00x0,0x00x0,0x00x1,0xff0xff,0x4e0x0,0x10x0,0x00x0,0x10x19,0x00x20,0x750x6e,0x6b0x6e,0x6f0x77,0x6e0x20,0x410x42,0x490x2c,0x200x6d,0x640x6d,0x780x2c,0x200x6d,0x690x70,0x730x33,0x320x72,0x360x0,0x300x19,0x620x9d,0x5c0x0,0x10x29,0x480x42,0x420x42,0x420x42,0x420x42,0x420x42,0x420x42,0x420x42,0x420x42,0x20x0,0x7f0xfe,0xff0xff,0xff0xff,0x420x42,0x420xf3,0x2e0x53,0x430x4f,0x520x45,0x2e0x73,0x740x74,0x620x4e,0x500x82,0x00x0,0x5a0x63,0x9d0x5e,0x10x0,0x410x54,0x450x0,0x7f0x45,0x4c0x46,0x7f0x46,0x4e0x43,0x90x0,0x00x0,0x00x22,0x3e0x54,0x7f0x45,0x140x0,0xd0x0,0x00x0,0x800x0,0x00x0,0xff0x90,0xd30xa5,0x40x0,0x00x0",
        "binutils strings": "0x00xa,0x20x0,0x00x0,0x10xff,0xff0x0,0x00x12,0xff0x18,0x00x0,0x00x0,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x0,0x00x3c,0x00x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x2f0x2f,0x00x0,0x00x0,0x00x0,0xa0x17,0x00xac,0x880x0,0x00x0,0x6e0x75,0x8c0x65,0x720x69,0x570x22,0x4d0x0,0x00x0,0x30x0,0x00x0,0x20xff,0x00x0,0x00x0,0x800x7f,0xf0x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x0,0x00x2,0x00x0,0x4d0x4d,0xff0x80,0x490x47,0xc80x49,0x10x7,0x00x0,0xff0xff,0xff0xdf,0x00x0,0x00x0,0xff0xe0",
        "grok": "0x00x0,0x00xc,0x6a0x50,0x200x20,0xd0xa,0x870xa,0x00x0,0x00x14,0x660x74,0x790x70,0x6a0x77,0x440x9,0x10x1,0x10x1,0x670x6e,0x340x9,0x00x0,0x00x5b,0x6a0x70,0x320x68,0x00x0,0x00x16,0x690x68,0x640x72,0x10x1,0x10x9,0x10x1,0x10x20,0x00x4,0xff0x7,0x10x1,0x00x0,0x00xc,0x620x70,0x630x63,0x1b0x1b,0x1b0x1,0x00x0,0x00xf,0x630x6f,0x6c0x72,0x10x1,0x10x1,0x00x0,0x100x0,0x00x0,0x220x63,0x640x65,0x660x0,0x40x0,0x00x0,0x00x0,0x00x0,0x10x0,0x00x0,0x20x0,0x20x0,0x00x0,0x30x0,0x30x0,0x10x0,0x00x1,0x10x0,0x250x6a,0x700x32,0x630xff,0x4f0xff,0x510x0,0x320x0,0x10x0,0x00x0,0x200x0,0x00x0,0x200x0,0x00x0,0xb0x0,0x00x0,0x00x0,0x00x0,0x200x0,0x00x0,0x200x0,0x00x0,0x00x0,0x00x0,0x00x0,0x40x4,0x10x1,0x40x9,0x10x4,0x10x1,0x00x1,0x10xff,0x520x0,0xc0x0,0x10x1,0x00x0,0x50x4,0x40x1,0x10xff,0x5c0x0,0x130x40,0x290x31,0x310x37,0x310x31,0x350x31,0x310x39,0x310x31,0x360x31,0x310x32,0xff0x64,0x00x25,0x00x1,0x420x6c,0x660x64,0x6e0x62,0x630x9,0x630x6b,0x90x4b,0x750x62,0x680x48,0x530x46,0x4a0x9,0x740x63,0x790x6a,0x6e0x72,0x740x9,0x350x2d,0xe0x2c,0x310xff,0x900x1,0xd0x1,0x10x1,0x10x1,0xe60x1,0x00x0,0xec0xbe,0x170x1c,0xb00xff,0xf0x20,0xbc0xfe,0x1c0xff,0xde0x8f,0x1d0x1b,0xd80xb9,0x170x19,0xc00xd0,0xdb0xf,0x200xa8,0xb70x3a,0x1c0xdc,0x9c0x17,0x1c0xe8,0xda0x4e,0xd00xdb,0x90x14,0x10x59,0x100xde,0x9f0xbf,0x140x3d,0xa80xff,0xdc0x9b,0xd0x1,0xb00xaa,0x400x0,0xd80xb1,0x30x1c,0x00xa2,0x420xa3,0x290x0,0xd80xb1,0x30x1c,0x00xa2,0x520x8d,0x7d0x0,0xff0xde,0x8f0xf5,0x400x65,0xdd0xd6,0x2c0xdc,0x9b0xd,0x400xd9,0x370x57,0xa60xe1,0xa0xdd,0xf90xdc,0x9c0xf,0x3e0xd9,0x320x57,0xa60xe1,0xa0xc0,0xff0xdf,0x970xbf,0xe20xe9,0xd70x87,0x40x5b,0xde0x8f,0xf10xe2,0xd30x24,0x3c0x8a,0x590xa9,0xde0x8f,0xf30xe2,0xd30x28,0x3e0x86,0x760xff,0x00xa6",
    }

    seed_queue = [example[args.fuzzing_target]]

    generation_kwargs = {
        "do_sample": True,
        "min_length": -1,
        "top_p": 0.92,  # 0.9
        "top_k": 50,
        "temperature": args.temperature,
        "pad_token_id": tokenizer.bos_token_id,
    }

    while True:
        global seeds_from_fuzzer
        is_from_fuzzer = False
        current_seed = random.choice(seed_queue)

        if seeds_from_fuzzer:
            current_seed = seeds_from_fuzzer.pop()
            if len(seed_queue) > 30:
                seed_queue = []
            seed_queue.append(current_seed)
            is_from_fuzzer = True

        formatted_chunks = []
        if not args.if_text:
            for i in range(0, len(current_seed), 4):
                if i + 3 < len(current_seed):
                    formatted_chunks.append(
                        f"0x{current_seed[i:i+2]}0x{current_seed[i+2:i+4]}"
                    )
                else:
                    # If no pair, add the single element
                    formatted_chunks.append(f"0x{current_seed[i:]}")
        else:
            formatted_chunks.append(current_seed)
        prompt = (
            "### Input: ```Based on below hex "
            + args.fuzzing_target
            + " seed, mutate a new "
            + args.fuzzing_target
            + " seed. Make sure the example is complete and valid. "
            + ",".join(formatted_chunks)
            + "```"
        )

        query_tensors = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")
        response_tensors = model.generate(
            input_ids=query_tensors,
            max_new_tokens=500,
            **generation_kwargs,
        )

        response = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)
        # Compute sentiment score
        global uid, seed_id_map, id_rwd_map, message_queue
        for r in response:
            seed = hex_string_to_hex(r, args.fuzzing_target, args.if_text)
            seed_id_map[seed] = uid + os.getpid()
            # id_rwd_map[uid + os.getpid()] = float(0.0)
            message_queue.append(seed)
            if is_from_fuzzer:
                print("sff:::", seed[:15])
            else:
                print("seed:::", seed[:15])
        uid += 8
        torch.cuda.empty_cache()


if __name__ == "__main__":
    t = threading.Thread(
        target=mq_thread,
        args=(),
    )
    t.start()
    t2 = threading.Thread(target=mq_thread2, args=())
    t2.start()
    # if accelerator.is_main_process:
    # t2 = threading.Thread(target=reward_thread, args=())
    # t2.start()
    # time.sleep(7200)
    main()
